---
title: 'Lab 2: Linear Regression'
author: "Lizzy Presland"
date: "January 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Exploring and preparing the data

The data is taken from the .csv file posted on the class Canvas page.

#### Q1: Load the data

To load the data, we would normally use: 

```
ins_data <- read.csv("insurance.csv", header=TRUE)
```

As far as I can tell, we can load data with `stringsAsFactors=TRUE` in order for R to correctly parse the file and load a simple dataframe. However, we will want to use `stringsAsFactor=FALSE` in order to perform ***linear regression analysis*** on the data.

Therefore, our load function becomes:

```{r include=TRUE}
ins_data <- read.csv("insurance.csv", header=TRUE, stringsAsFactors = TRUE)
```

#### Q2: Data structure

```{r}
head(ins_data)
summary(ins_data)
```

#### Q3: Independent and dependent variables

The dependent feature is the **charges** column (the amount of money the insurance company can expect to pay for healthcare costs); the independent features are all others (**age**, **sex**, **bmi**, **number of children**, **tobacco use**, and **geographic region**).

#### Q4: Dependent feature analysis

It is best to analyze the dependent feature numerically; I shall use graphical and numerical methods.

#### Q5: Determine data's normal distribution

#### Histogram

```{r}
hist(ins_data$charges)
```

As the histogram shows, the data does not appear to be normally distributed; there appears to be a strong right skew to the data.

#### Numerical Methods

```{r include=FALSE}
cat("Mean: ")
mean(ins_data$charges)
cat("\nMedian: ")
median(ins_data$charges)
```

## Exploring relationships among features using a correlation matrix

#### Q7: create the correlation matrix

```{r}
numerics <- ins_data[,c(1, 3, 4, 7)]
head(numerics)
cor(numerics, numerics)
```

**NOTE:** It was tricky getting all 4 numerical values into a matrix as described, and took some intuition to figure out what was desired. It would be helpful to review this in class.

#### Q8: reflect on patterns in matrix

The two categories with the closest relationship are **age** and **charges**, which isn't surprising given that conventional wisdom dictates that healthcare costs increase with age.

The next most significant correlation is between **BMI** (body mass index) and **charges**, which is unsurprising for similar reasons; according to conventional wisdom, overweight people are more likely to have health problems and/or spend more money on healthcare.

## Visualizing relationships using a scatterplot matrix

#### Q9: the scatterplot matrix

```{r}
pairs(numerics)
```

#### Q10: Interesting observations in scatterplots

I found the scatterplot of the BMI and charges to be particularly interesting.

```{r}
plot(numerics$bmi, numerics$charges)
```

There are clustering patterns of high and low charges for the same BMI range (approx 30-40 BMI). The visual disparity of these two clusters indicate that there is a pattern of healthcare spending for these individuals, and because the data is not evenly distributed through the cost spectrum in this range of BMI values, there may be an underlying pattern or set of circumstances dictating this behavior.

#### Q11: enhanced scatterplot matrix

```{r}
library("psych")
pairs.panels(numerics)
```

#### Q11 Continued: new observations with enhanced scatterplot

* There appears to be a strong correlation between age and charges per the loess curve.
* BMI appears to be normally distributed
* Age is approximately evenly distrubted
* Nmber of children & total charges are heavily right-skewed
* The average number of children is quite low (around 1) for the sampled population.
* For all other variables besides charges, the mean charges is quite low on the linear scale of dollar amounts (per the axis ranges).
* Lastly, this is a super informative plot! Very cool.

## Training a model on the data

#### Q12: Generate a model in R

```{r}
ins_model <- lm(charges ~ age + sex + smoker + bmi + children + region, data=ins_data)
```

#### Q13: Show estimated beta coefficients

```{r}
ins_model
```

* There are 8 coefficients listed instead of 6 because each nominal category in the "Region" data column is listed as a separate feature/variable.
* The beta value for each feature is the linear rate of change of `y` for each unit of change in the indepenent variable `x_i`. For example, each point increase in `BMI` causes the dependent variable `charges` to increase approximately $339.2.

#### Q14: What increase or decrease will affect the medical expenses for each additional child? Or each additional year of age? Or unit increase of BMI?

Based on the coefficient values:
* Each additional child will cause charges to increase approximately $475.00 
* Each additional year of age will cause charges to increase approximately $257.00
* Each additional point increase in the example's BMI will cause charges to increase approximately $340.00

#### Q15: Explain the effect of medical expenses in numbers on each one of the 3 factor features.

* For the `sex` (gender) feature, the average cost will decrease when the example is male (relative to a female example).
* For the `smoker` feature, the average cost will increase dramatically (about $23,000 dollars).
* For the `region` feature, the enumerated values (`regionnorthwest`, `regionsoutheast`, and `regionsouthwest`) each result in a decrease in cost relative to the baseline category (deduced to be `regionnortheast`).

## Evaluating model performance

#### Q16: Evaluate model using `summary()`

```{r}
summary(ins_model)
```

#### Q17: Reflect on the model's accuracy in context of important performance indicators

| Performance Indicator | Result |
|-----------------------|--------|
| P-Value | The p-values for `bmi`, `smokeryes`, and `age` are all extremely small, which suggests that these factors likely have a strong relationship with the dependent variable. Additionally, the `intercept` p-value is very small, indicating that the model is likely performing well. |
| Residuals | The range of the residuals is very large (about \$40,000), but the range of the 2nd & 3rd quartiles is much more reasonable (approx \$4,000). This relatively small cumulative residual amount for the majority of examples, along with a median residual of less than \$1,000, indicates the model is not wildly inaccurate (if you consider the middle half of the margins of error to be within an acceptable range of error). |
| R-squared Value | The r-squared value is approximately 0.75 for the model; in most cases, an r-squared value of 0.5 or greater indicates that the model is predicting the dependent variable's value with good or acceptable accuracy. This also suggests that our model is performing well. |

## Improving the Model

First, the `age` feature will be described as a non-linear (polynomial/quadratic) relationship with the dependent variable instead of a linear relationship.

```{r}
ins_data$age2 <- ins_data$age^2
```

Next, the BMI numeric value will be converted to a binary indicator.

```{r}
ins_data$bmi30 <- ifelse(ins_data$bmi >= 30, 1, 0)
```

#### Q18: Train the Improved Model

Finally, we include an interaction between high BMI and smoking in our new model.

```{r}
ins_model_v2 <- lm(charges ~ age2 + sex + smoker + bmi30 + bmi30*smoker + children + region, data=ins_data)
```

```{r}
ins_model_changes_only <- lm(charges ~ bmi30*smoker, data = ins_data)
```

```{r}
summary(ins_model_v2)
summary(ins_model_changes_only)
```

Reflections:

* The improved model (that included the non-linear age relationship and the interaction between BMI and smoking usage) significantly outperformed the original model, with an R-squared value of ~0.86.
* I wanted to test the training of a model using only the described changes, ans was somewhat surprised that there was almost no gain in performance when limiting the feature set in the model.
* In the high-performing improved model, the intercept's p-value decreased slightly and the inter-quartile range (1Q-3Q) residual decreased substantially (to about $600 instead of $4000).
* In the improved model, the significant p-value of BMI disappeared, but the interaction feature between BMI and smoker became a significant p-value in its place.