---
title: 'Lab 3: KNN'
author: "Lizzy Presland"
date: "January 30, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploring and Preparing the Data

```{r}
bc_data = read.csv("wisc_bc_data.csv", header=TRUE)
```

Here's some interesting summarizations of the data:

```{r}
head(bc_data)
summary(bc_data)
```

We will exclude the patient identifier (in the first column of the dataframe); this is done for two reasons:

* The ID is not a feature of interest for our k-NN model.
* More importantly, it is very important to remove personal identifiers from sensitive datasets like this, so that the subject's privacy may be respected.

```{r}
bc_data[1] <- NULL
```

The target variable is **Diagnosis Type**, now listed in column 1. There are two levels: Benign (**B**), and Malignant (**M**).

This column of data needs to be factorized, and we will rename the shorthand **B** and **M** labels to something more informative.

```{r}
bc_data$diagnosis <- as.factor(bc_data$diagnosis)
levels(bc_data$diagnosis)[levels(bc_data$diagnosis)=="B"] <- "benign"
levels(bc_data$diagnosis)[levels(bc_data$diagnosis)=="M"] <- "malignant"
summary(bc_data$diagnosis)
```

Now, the percentage occurance of malignant and benign diagnoses can be calculated.

```{r}
library(plyr)
summarize( bc_data, "Frequency"=count(bc_data$diagnosis), "Percentage"=100*count(bc_data$diagnosis)/nrow(bc_data))
# A better method for percentages:
# round(prop.table(table(bc_data$diagnosis)) * 100, digits = 1)
# Better Method Implemented:
round(prop.table(table(bc_data$diagnosis)) * 100, digits = 1)
```

Summary of these 3 features:

```{r}
summary(bc_data$radius_mean)
summary(bc_data$area_mean)
summary(bc_data$smoothness_mean)
```

These features could be problematic because their numerical values are so disparate; if the model looks at numeric values with size being the largest determining factor across different features, then a feature that has larger units or values could be considered more significant or relevant to the outcome than others whose data spread/values are smaller (but is a more significant indicator for classification purposes).

For this reason, we want to normalize the data in a `[0, 1]` range.

## Transformation and Normalization

The normalize() function is defined as follows:

```{r}
normalize <- function(x){
  min <- min(x)
  range <- max(x) - min
  for (idx in 1:length(x)) {
    x[idx] <- (x[idx] - min) / range
  }
  return(x)
}
```

To test, we will create a simple vector:

```{r}
vals <- c(1, 2, 3, 4, 5)
vals <- normalize(vals)
vals
```

Now that the function has been tested, normalize() will be applied to each numerical feature in the dataset. Because the first column of the dataset is a list of factors, the first column will be excluded.

```{r}
wbcd_norm <- as.data.frame(lapply(bc_data[-1], normalize))
```

Now, our data looks like this:

```{r}
head(wbcd_norm)
```

## Create the Datasets for Training

We will slice the data sets into two chunks for training and testing.

```{r}
wbcd_train <- wbcd_norm[1:469,]
wbcd_test <- wbcd_norm[470:569,]
```

Next, we will store the target variable for each corresponding dataset as a vector. Because the feature we wish to train the model on is not included in the normalized dataset we created, we must extract it from the original dataframe (and be careful to slice the dataframe with the same indeces we used to create the training & test datasets).

```{r}
wbcd_train_labels <- bc_data[1:469,]$diagnosis
wbcd_test_labels <- bc_data[470:569,]$diagnosis
```

## Training a Model on the Data

We will use the k-NN implementation from R's `class` package. We import it like so:

```{r}
library(class)
```

If it is not previously installed, you may install the `class` package in R Studio by going to `Tools -> Install Packages...`, entering the package name `class` into the `Packages` prompt, and installing it to your default path.

##### Finding K

Without any other deciding factors influencing what K value is needed for the number of nearest neighbors evaluated, the best option is to use the square root of the training dataset size. In this case, because there are 469 units of observation included in our training dataset, I will initially use a K value of 22.

##### Create the Model

Here, the call to create the model is done:

```{r}
wbcd_test_pred <- knn(wbcd_train, wbcd_test, wbcd_train_labels, 22)
```

## Evaluating the Model's Performance

Let's look at the vectors for the test data's target feature compared with the model's prediction.

```{r}
prediction_table <- data.frame(wbcd_test_pred, wbcd_test_labels)
prediction_table
```

There are a few ways to evaluate the performance of the model; we primarily want to compare the test data's true labels (contained in `wbcd_test_labels`) with the `knn()` call output (contained in `wbcd_test_pred`).

First, we examine the relationship between the two vectors using Pearson's Chi-squared test.

The premise of a Chi-squared test is the null hypothesis: there is no relationship between the two variables. In this example, the null and alternative hypotheses can be stated as:

* Null Hypothesis: There is no relationship between the model's prediction and the actual value of the target variable.
* Alternative Hypothesis: There is a statistically significant relationship between the model's prediction and the actual value of the target variable; the model's predicted value is a good prediction of the actual value.

In this test, we can only choose to reject the null hypothesis. We can make this determination from the Chi-square test's p-value; a p-value that is less than our pre-determined significance level indicates that the null hypothesis can be rejected and there is a significant correlation between the two variables. For this example, we choose a significance level of 0.05.

```{r}
chisq.test(wbcd_test_labels, wbcd_test_pred)
```

Based on the Chi-squared test, this model is very accurate because the p-value is so small (less than `2.2e-16`).

Let us now examine the individual items whose predicted results were different from the actual value. We will examine two different possibilities:

* False positives (where prediction resulted in `malignant` label but the actual value was `benign`)
* False negatives (where actual value was `malignant` but the prediction was `benign`)

The code snippet below generates the dataframes for false positives and false negatives.

```{r}
false_positives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
false_negatives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
for (idx in 1:100) {
  if (wbcd_test_pred[idx] == "malignant" && wbcd_test_labels[idx] == "benign") {
    new_row <- data.frame(Index=idx, Predicted=wbcd_test_pred[idx], Actual=wbcd_test_labels[idx])
    false_positives <- rbind(false_positives, new_row)
  }
  if (wbcd_test_pred[idx] == "benign" && wbcd_test_labels[idx] == "malignant") {
    new_row <- data.frame(Index=idx, Predicted=wbcd_test_pred[idx], Actual=wbcd_test_labels[idx])
    false_negatives <- rbind(false_negatives, new_row)
  }
}
```

Now that we have two data frames reflecting our false positives and negatives, let's examine these subsets:

```{r}
false_positives
false_negatives
```

With the k-value of 22 (roughly `sqrt(N=469)`), no false positives were predicted, but two false negatives were generated.

In actual usage, false negatives are far more dangerous for cancer patients, because a malignant tumor was incorrectly assumed to be benign, and the patient will not receive further diagnosis steps and treatment options for a malignant disease.

There may be a few ways of improving model performance; examining the model's performance with other k-values would be a useful start, and other options include:

* Testing the model's performance when values aren't normalized
* Use a weighted voting process with a larger k-value, so that the Euclidian distance between the nearest neighbors emphasizes relationships that are closer to the data point rather than treating all K items with equal importance

We will explore some of these options further in the next section.

## Improving Model Performance

##### Question 1: How can the model's performance be improved?

There are two possible approaches to improving the model's performance that seem fruitful:

* Using different k-values
* Using a weighted approach for nearest neighbors that are close (when the k-value is larger)

### Applying the Improved Methods

#### Method 1: Testing alternate k-values

For simplicity, I will perform the following operations for k-values `[1, 160]`:

* Create the k-NN model and create a prediction vector
* Analyze the prediction vector for false positives and false negatives
* Create a dataframe for the false positives & false negatives for each prediction vector

Lastly, I will generate a chart showing the error trends for these operations.

First, let's define some utility functions:

```{r}
get_false_tables <- function(predictions) {
  false_positives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
  false_negatives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
  for (idx in 1:100) {
    if (predictions[idx] == "malignant" && wbcd_test_labels[idx] == "benign") {
      new_row <- data.frame(Index=idx, Predicted=predictions[idx], Actual=wbcd_test_labels[idx])
      false_positives <- rbind(false_positives, new_row)
    }
    if (predictions[idx] == "benign" && wbcd_test_labels[idx] == "malignant") {
      new_row <- data.frame(Index=idx, Predicted=predictions[idx], Actual=wbcd_test_labels[idx])
      false_negatives <- rbind(false_negatives, new_row)
    }
  }
  results <- list(nrow(false_positives), nrow(false_negatives))
  return(results)
}

```

Now, we use the utility function(s) to collect false positive & false negative information for each value of k.

```{r}
false_counts <- data.frame(K=integer(), FalsePositives=integer(), FalseNegatives=integer(), PVal=double())
for (k in 1:160) {
  wbcd_test_pred <- knn(wbcd_train, wbcd_test, wbcd_train_labels, k)
  results <- get_false_tables(wbcd_test_pred)
  # TODO: fix placeholder p-val
  new_row <- data.frame(K=k, FalsePositives=results[1], FalseNegatives=results[2], PVal=0.0)
  false_counts <- rbind(false_counts, new_row)
}
```

Let's confirm that the output looks okay:

```{r}
head(false_counts)
```

