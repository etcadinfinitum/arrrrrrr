---
title: 'Assignment 3: KNN Performance Improvements'
author: "Lizzy Presland"
date: "January 30, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploring and Preparing the Data

```{r}
bc_data = read.csv("wisc_bc_data.csv", header=TRUE)
```

Here's some interesting summarizations of the data:

```{r}
head(bc_data)
summary(bc_data)
```

We will exclude the patient identifier (in the first column of the dataframe); this is done for two reasons:

* The ID is not a feature of interest for our k-NN model.
* More importantly, it is very important to remove personal identifiers from sensitive datasets like this, so that the subject's privacy may be respected.

```{r}
bc_data[1] <- NULL
```

The target variable is **Diagnosis Type**, now listed in column 1. There are two levels: Benign (**B**), and Malignant (**M**).

This column of data needs to be factorized, and we will rename the shorthand **B** and **M** labels to something more informative.

```{r}
bc_data$diagnosis <- as.factor(bc_data$diagnosis)
levels(bc_data$diagnosis)[levels(bc_data$diagnosis)=="B"] <- "benign"
levels(bc_data$diagnosis)[levels(bc_data$diagnosis)=="M"] <- "malignant"
summary(bc_data$diagnosis)
```

Now, the percentage occurance of malignant and benign diagnoses can be calculated.

```{r}
library(plyr)
summarize( bc_data, "Frequency"=count(bc_data$diagnosis), "Percentage"=100*count(bc_data$diagnosis)/nrow(bc_data))
# A better method for percentages:
# round(prop.table(table(bc_data$diagnosis)) * 100, digits = 1)
# Better Method Implemented:
round(prop.table(table(bc_data$diagnosis)) * 100, digits = 1)
```

Summary of these 3 features:

```{r}
summary(bc_data$radius_mean)
summary(bc_data$area_mean)
summary(bc_data$smoothness_mean)
```

These features could be problematic because their numerical values are so disparate; if the model looks at numeric values with size being the largest determining factor across different features, then a feature that has larger units or values could be considered more significant or relevant to the outcome than others whose data spread/values are smaller (but is a more significant indicator for classification purposes).

For this reason, we want to normalize the data in a `[0, 1]` range.

## Transformation and Normalization

The normalize() function is defined as follows:

```{r}
normalize <- function(x){
  min <- min(x)
  range <- max(x) - min
  for (idx in 1:length(x)) {
    x[idx] <- (x[idx] - min) / range
  }
  return(x)
}
```

To test, we will create a simple vector:

```{r}
vals <- c(1, 2, 3, 4, 5)
vals <- normalize(vals)
vals
```

Now that the function has been tested, normalize() will be applied to each numerical feature in the dataset. Because the first column of the dataset is a list of factors, the first column will be excluded.

```{r}
wbcd_norm <- as.data.frame(lapply(bc_data[-1], normalize))
```

Now, our data looks like this:

```{r}
head(wbcd_norm)
```

## Create the Datasets for Training

We will slice the data sets into two chunks for training and testing.

```{r}
wbcd_train <- wbcd_norm[1:469,]
wbcd_test <- wbcd_norm[470:569,]
```

Next, we will store the target variable for each corresponding dataset as a vector. Because the feature we wish to train the model on is not included in the normalized dataset we created, we must extract it from the original dataframe (and be careful to slice the dataframe with the same indeces we used to create the training & test datasets).

```{r}
wbcd_train_labels <- bc_data[1:469,]$diagnosis
wbcd_test_labels <- bc_data[470:569,]$diagnosis
```

## Training a Model on the Data

We will use the k-NN implementation from R's `class` package. We import it like so:

```{r}
library(class)
```

If it is not previously installed, you may install the `class` package in R Studio by going to `Tools -> Install Packages...`, entering the package name `class` into the `Packages` prompt, and installing it to your default path.

##### Finding K

Without any other deciding factors influencing what K value is needed for the number of nearest neighbors evaluated, the best option is to use the square root of the training dataset size. In this case, because there are 469 units of observation included in our training dataset, I will initially use a K value of 22.

##### Create the Model

Here, the call to create the model is done:

```{r}
wbcd_test_pred <- knn(wbcd_train, wbcd_test, wbcd_train_labels, 22)
```

## Evaluating the Model's Performance

Let's look at the vectors for the test data's target feature compared with the model's prediction.

```{r}
prediction_table <- data.frame(wbcd_test_pred, wbcd_test_labels)
prediction_table
```

There are a few ways to evaluate the performance of the model; we primarily want to compare the test data's true labels (contained in `wbcd_test_labels`) with the `knn()` call output (contained in `wbcd_test_pred`). It is also beneficial to look at the model's overall accuracy; we shall do this second (after false positives and false negatives have been found).

#### Evaluating Accuracy using Chi-squared test of Prediction & Actual Vectors

Next, we examine the relationship between the two vectors using Pearson's Chi-squared test.

The premise of a Chi-squared test is the null hypothesis: there is no relationship between the two variables. In this example, the null and alternative hypotheses can be stated as:

* Null Hypothesis: There is no relationship between the model's prediction and the actual value of the target variable.
* Alternative Hypothesis: There is a statistically significant relationship between the model's prediction and the actual value of the target variable; the model's predicted value is a good prediction of the actual value.

In this test, we can only choose to reject the null hypothesis. We can make this determination from the Chi-square test's p-value; a p-value that is less than our pre-determined significance level indicates that the null hypothesis can be rejected and there is a significant correlation between the two variables. For this example, we choose a significance level of 0.05.

```{r}
chisq_result <- chisq.test(wbcd_test_labels, wbcd_test_pred)
chisq_result
```

Based on the Chi-squared test, this model is very accurate because the p-value is so small (less than `2.2e-16`).

#### Inspecting Type I and Type II Errors

Let us now examine the individual items whose predicted results were different from the actual value. We will examine two different possibilities:

* False positives (where prediction resulted in `malignant` label but the actual value was `benign`); this is a **Type I** error
* False negatives (where actual value was `malignant` but the prediction was `benign`); this is a **Type II** error

The code snippet below generates the dataframes for false positives and false negatives.

```{r}
false_positives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
false_negatives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
true_positives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
true_negatives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
for (idx in 1:100) {
  if (wbcd_test_pred[idx] == "malignant" && wbcd_test_labels[idx] == "benign") {
    new_row <- data.frame(Index=idx, Predicted=wbcd_test_pred[idx], Actual=wbcd_test_labels[idx])
    false_positives <- rbind(false_positives, new_row)
  }
  else if (wbcd_test_pred[idx] == "benign" && wbcd_test_labels[idx] == "malignant") {
    new_row <- data.frame(Index=idx, Predicted=wbcd_test_pred[idx], Actual=wbcd_test_labels[idx])
    false_negatives <- rbind(false_negatives, new_row)
  }
  else if (wbcd_test_pred[idx] == "benign") {
    new_row <- data.frame(Index=idx, Predicted=wbcd_test_pred[idx], Actual=wbcd_test_labels[idx])
    true_negatives <- rbind(true_negatives, new_row)
  } else {
    new_row <- data.frame(Index=idx, Predicted=wbcd_test_pred[idx], Actual=wbcd_test_labels[idx])
    true_positives <- rbind(true_positives, new_row)
  }
}
```

Now that we have two data frames reflecting our false positives and negatives, let's examine these subsets:

```{r}
false_positives
false_negatives
```

With the k-value of 22 (roughly `sqrt(N=469)`), `r nrow(false_positives)` false positives were predicted, but `r nrow(false_negatives)` false negatives were generated.

In actual usage, false negatives are far more dangerous for cancer patients, because a malignant tumor was incorrectly assumed to be benign, and the patient will not receive further diagnosis steps and treatment options for a malignant disease.

#### Calculating Accuracy and Error of the Model

Lastly, we will examine overall accuracy of the model.

```{r}
error_rate <- (nrow(false_negatives)+nrow(false_positives)) / (nrow(false_negatives) + nrow(false_positives) + nrow(true_negatives) + nrow(true_positives))
accuracy <- 1 - error_rate
error_rate
accuracy
```

These calculations indicate that the model is has an accuracy of `r accuracy * 100`% and has an error rate of `r error_rate * 100`%, which is reasonable.

#### Improving the Model

There may be a few ways of improving model performance; examining the model's performance with other k-values would be a useful start, and other options include:

* Testing the model's performance when values aren't normalized
* Use a weighted voting process with a larger k-value, so that the Euclidian distance between the nearest neighbors emphasizes relationships that are closer to the data point rather than treating all K items with equal importance

We will explore some of these options further in the next section.

## Improving Model Performance

##### Question 1: How can the model's performance be improved?

There are several possible approaches to improving the model's performance that seem fruitful:

* Using different k-values
* Using a weighted approach for nearest neighbors that are close (when the k-value is larger)
* Using a k-NN library that incorporates cross-validatory classification
* Utilizing z-score data normalization instead of the standard normalization method used up until now.

I will focus on the first two approaches (different k-values and weighted k-NN methods).

### Applying the Improved Methods

#### Method 1: Testing alternate k-values

###### Model Creation

For simplicity, I will perform the following operations for k-values `[1, 160]`:

* Create the k-NN model and create a prediction vector
* Analyze the prediction vector for false positives and false negatives
* Create a dataframe for the false positives & false negatives for each prediction vector

Lastly, I will generate a chart showing the error trends for these operations.

First, let's define some utility functions:

```{r}
get_false_tables <- function(predictions) {
  false_positives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
  false_negatives <- data.frame(Index=integer(), Predicted=factor(), Actual=factor())
  for (idx in 1:100) {
    if (predictions[idx] == "malignant" && wbcd_test_labels[idx] == "benign") {
      new_row <- data.frame(Index=idx, Predicted=predictions[idx], Actual=wbcd_test_labels[idx])
      false_positives <- rbind(false_positives, new_row)
    }
    if (predictions[idx] == "benign" && wbcd_test_labels[idx] == "malignant") {
      new_row <- data.frame(Index=idx, Predicted=predictions[idx], Actual=wbcd_test_labels[idx])
      false_negatives <- rbind(false_negatives, new_row)
    }
  }
  results <- list(nrow(false_positives), nrow(false_negatives))
  return(results)
}

```

Now, we use the utility function(s) to collect false positive & false negative information for each value of k.

```{r}
false_counts <- data.frame(K=integer(), FalsePositives=integer(), FalseNegatives=integer(), PVal=double(), Error=double())
for (k in 1:160) {
  wbcd_test_pred <- knn(wbcd_train, wbcd_test, wbcd_train_labels, k)
  results <- get_false_tables(wbcd_test_pred)
  chisq_result <- chisq.test(wbcd_test_labels, wbcd_test_pred)
  new_row <- data.frame(K=k, FalsePositives=results[[1]], FalseNegatives=results[[2]], PVal=chisq_result$p.value, Error=((results[[1]] + results[[2]]) / 100))
  false_counts <- rbind(false_counts, new_row)
}
```

##### Accuracy for variable-K 

Let's plot the overall accuracy of each model (as a percentage figure):

```{r}
plot(false_counts$K, 100 - (false_counts$Error * 100))
```

The model starts to lose its high accuracy alues at around `k=25`, and decreases as `k` increases (an unsurprising result but still worth noting).

##### Type I and Type II errors for models with varying K

To get a visual representation of model performance relative to Type I and Type II errors, let's plot the number of false positives and false negatives for each k.

First, false positives:

```{r}
plot(false_counts$K, false_counts$FalsePositives)
```

All false positive results are clustered at the very low end of the range of k values, which indicates that false negatives occur when the value of k is low and the network does not have enough nearest neighbors to correctly associate biopsy traits with a benign tumor instead of a malignant one.

However, as stated before, false negatives are the results that we are more concerned with. Recall that in actual usage, false negatives are far more dangerous for cancer patients; when a malignant tumor is incorrectly assumed to be benign, the patient will not receive further diagnosis steps and treatment options for a malignant disease which will affect their life expectancy and allow the disease to progress further.

The scatterplot of false negatives for varying k is:

```{r}
plot(false_counts$K, false_counts$FalseNegatives)
```

Wow! There are way too many false negatives as k increases! Based on this data, the results with a k-value in `[1, 25]` will produce the lowest number of false negatives, which is our preferred outcome.

**These results indicate that smaller values for K lead to lower error rates.**

#### Method 2: Weighted k-NN Model

The library I will be using to explore weighted k-nearest neighbor classification is documented at https://cran.r-project.org/web/packages/kknn/kknn.pdf and requires the `kknn` package in R.

```{r}
library(kknn)
```

To test the approach, I will generate the same data for the weighted algorithm as I did in the previous section (see Method 1: Testing alternate k-values). This tests the weighted k-NN approach for the same range of k values. **Note:** I have chosen the parameters for the `kknn()` call after some experimentation, so these results are produced after many attempts at optimizing the `kknn` usage.

```{r}
kknn_false_counts <- data.frame(K=integer(), FalsePositives=integer(), FalseNegatives=integer(), PVal=double(), Error=double())
for (k_val in 1:160) {
  # the fancy new k-knn training method
  wbcd_test_pred <- kknn(wbcd_train_labels~., wbcd_train, wbcd_test, k=k_val, distance = 2, kernel="triangular")
  results <- get_false_tables(wbcd_test_pred$fitted.values)
  chisq_result <- chisq.test(wbcd_test_labels, wbcd_test_pred$fitted.values)
  new_row <- data.frame(K=k_val, FalsePositives=results[[1]], FalseNegatives=results[[2]], PVal=chisq_result$p.value, Error=((results[[1]] + results[[2]]) / 100))
  kknn_false_counts <- rbind(kknn_false_counts, new_row)
}
```

We are looking for an improvement in the false negatives values in this model; the false negatives are plotted here:

```{r}
plot(kknn_false_counts$K, kknn_false_counts$FalseNegatives, col="red", ylim=range(c(kknn_false_counts$FalseNegatives, false_counts$FalseNegatives)), xlab="K", ylab="False Negative Count", main="Type II Errors for Traditional k-NN and KKNN", pch=0)
par(new=TRUE)
plot(false_counts$K, false_counts$FalseNegatives, col="blue", axes=FALSE, xlab="", ylab="", pch=0)
legend(0, 8.5, c("KKNN", "Original k-NN"), col=c("red", "blue"), pch=0)
```

What about the model's accuracy across all k values?

```{r}
plot(kknn_false_counts$K, 100 - (kknn_false_counts$Error * 100), pch=2, col="red", xlab="K", ylab="Accuracy (%)", main="Accuracy of Traditional k-NN and KKNN")
par(new=TRUE)
plot(false_counts$K, 100 - (false_counts$Error * 100), pch=2, col="blue", axes=FALSE, xlab="", ylab="")
legend("bottomleft", c("KKNN", "Original k-NN"), col=c("red", "blue"), pch=2)
```

**These results indicate that while the overall accuracy of the KKNN model decreases for large K (relative to the traditional model), the KKNN model has fewer Type II errors as K increases (relative to the traditional model).** 